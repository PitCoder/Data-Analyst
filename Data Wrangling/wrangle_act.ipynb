{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling (WeRateDogs Data)\n",
    "\n",
    "![Image via Boston Magazine](https://d17h27t6h515a5.cloudfront.net/topher/2017/October/59dd378f_dog-rates-social/dog-rates-social.jpg)\n",
    "<div style=\"text-align: center\"><i>Image via <a href=\"https://www.bostonmagazine.com/arts-entertainment/2017/04/18/dog-rates-mit/\">Boston Magazine</i></div>\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#gathering\">Gathering</a></li>\n",
    "<li><a href=\"#assessing\">Assessing</a></li>\n",
    "<li><a href=\"#cleaning\">Cleaning</a></li>\n",
    "<li><a href=\"#iterating\">Iterating</a></li>\n",
    "<li><a href=\"#storing\">Storing</a></li>\n",
    "<li><a href=\"#acting\">Acting</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "<div style=\"text-align:justify\">\n",
    "<br>\n",
    "    <a href=\"https://twitter.com/dog_rates?lang=en\">WeRateDogs</a> is a Twitter account that rates people's dogs with a humorous comment about the dog. Their Twitter account data as many other real world cases does not comes clean, so as part of the Udacity's Data Analyst Nanodegree program the data wrangling process must be applied in order to obtain data that will be useful to create interesting and trustworthy analyses and visualizations of these rates.\n",
    "</div>\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Gathering: Gather data from three different sources in order to create a master dataset **(WeRateDogs Twitter archive, Image predictions file and Twitter API)**.\n",
    "\n",
    "- Assesing: Assess both visually and programatically for quality and tidiness issues. Detect and document at least **8 quality issues and 2 tidiness issues**.\n",
    "\n",
    "- Cleaning: Clean each of the issues you documented while assessing. **The result should be a high quality and tidy master pandas DataFrame (or DataFrames, if appropriate)**.\n",
    "\n",
    "- Storing: Store the clean DataFrame(s) in a CSV file with the main one. **Additionally, store the cleaned data in a SQLite database**.\n",
    "\n",
    "- Acting: Analyze and visualize the wrangled data, **producing at least 3 insights and 1 visualization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisite package imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "# library for HTTP request\n",
    "import requests\n",
    "\n",
    "# libaries for os interfaces interaction\n",
    "import os\n",
    "import filecmp\n",
    "\n",
    "# library for Twitter API\n",
    "#import tweepy\n",
    "#from tweepy import OAuthHandler\n",
    "\n",
    "# library to handle json\n",
    "import json\n",
    "import collections\n",
    "\n",
    "# library for sql database\n",
    "import sqlite3\n",
    "\n",
    "# package to calculate time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "## Gathering the Data\n",
    "\n",
    "The first source that will be gather is the WeRateDogs Twitter archive, this file has been downloaded manually from the following url: [Data source 1](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv).\n",
    "\n",
    "After the download, I transferred the file to the * src * folder located in the main directory of this notebook. Once there we read the file using the `read_csv` function from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'src'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "twitter_archive = pd.read_csv('src/twitter-archive-enhanced.csv')\n",
    "twitter_archive.head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we look that the data has been loaded properly by checking its info\n",
    "twitter_archive.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the second source that will be gather is the Image Predictions file, which is hosted on [Udacity's servers](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv) and should be programmatically downloaded, for this purpose we use the `request` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
    "#We here try to made a http GET request to obtain the files from the server\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print('Sucessful request')\n",
    "else:\n",
    "    print('The request was not sucessful:' + str(response.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the request was sucessful, now we proceed to save the tsv file\n",
    "with open(os.path.join(folder_name,\n",
    "                       url.split('/')[-1]), mode='wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the file has been sucessfully saved into the src folder, then we just\n",
    "# have to assert that the number of files in that directory\n",
    "assert len(os.listdir('src')) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Finally we just have to read the file, using pandas\n",
    "image_predictions = pd.read_csv('src/image-predictions.tsv', sep=\"\\t\")\n",
    "image_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we look that the data has been loaded properly by checking its info\n",
    "image_predictions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the third source will be gather using Twitter's API, the library `tweepy` allow us to to interact with the API using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet IDs for which to gather additional data via Twitter's API\n",
    "tweet_ids = twitter_archive.tweet_id.values\n",
    "len(tweet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The tokens for authetication must be HIDDEN for security reasons and\n",
    "# comply with Twitter's Terms and Conditions\n",
    "consumer_key = 'HIDDEN'\n",
    "consumer_secret = 'HIDDEN'\n",
    "access_token = 'HIDDEN'\n",
    "access_token_secret = 'HIDDEN'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Then we create an instance of the API\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we extract the ids from twitter-archive-enhanced dataset\n",
    "query_ids = twitter_archive['tweet_id'].values\n",
    "tweets = len(query_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Once having all the ids, we query Twitter's API for information from each Tweet\n",
    "# Twitter will return JSON data, so we must dump it into a file\n",
    "\n",
    "# These variables will help to keep on check on how is going the process\n",
    "# and retrieve failed tweets\n",
    "gauge = 0\n",
    "start = timer()\n",
    "failed_tweets = {}\n",
    "\n",
    "with open('src/tweets_json.txt', mode='w') as output:   \n",
    "    for tweet_id in query_ids:\n",
    "        gauge += 1\n",
    "        # Progress message\n",
    "        print(\"Retrieving: \" + str(tweet_id) + \n",
    "              \"  Missing: \" + str(tweets-gauge))\n",
    "        try:\n",
    "            # Get the tweet query status\n",
    "            # Pass in 'extended' to get non truncated tweet text\n",
    "            tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "            print(\"Sucess\")\n",
    "            json.dump(tweet.json, output)\n",
    "            output.write(\"\\n\")\n",
    "        # In case that Twitter responds with an error\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Failure\")\n",
    "            # We make a dictionary with the tweets that throwed errors\n",
    "            # for future handling\n",
    "            failed_tweets[tweet_id] = e\n",
    "            pass\n",
    "end = timer()\n",
    "print(\"Total Time: \" + str(end-start))\n",
    "print(\"Errors:\")\n",
    "print(failed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we read the dumped txt file, and append each tweet in a list\n",
    "tweets = []\n",
    "for line in open('src/tweets_json.txt', 'r'):\n",
    "    tweets.append(json.loads(line))\n",
    "# We verify the amount of tweets\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we explore the structure of the tweet in order to find the information that we want:\n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Once the tweets have been appended to the list, we extract the additional data that we want so we can build a dictionary, and finally construct the dataframe. The variables that will be extracted are:\n",
    "- *id*\n",
    "- *retweet_count*\n",
    "- *favorite_count*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We create lists that will hold each variable of the tweets\n",
    "tweet_ids = []\n",
    "retweet_counts = []\n",
    "favorite_counts = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet_ids.append(tweet['id'])\n",
    "    retweet_counts.append(tweet['retweet_count'])\n",
    "    favorite_counts.append(tweet['favorite_count'])\n",
    "    \n",
    "# Now we construct the dataframe from a dictionary with the previous data\n",
    "data = {'tweet_id':tweet_ids, 'retweet_count':retweet_counts, 'favorite_count':favorite_counts} \n",
    "tweets_counts = pd.DataFrame(data)\n",
    "tweets_counts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we have all the sources and data required for this project, we proceed to assess each dataset in order to find quality and tideness issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assessing'></a>\n",
    "## Assesing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Assessment\n",
    "The very first step when assessing the data is to do it visually, even though this might not be effective for large datasets, it will help us to get acquainted with the data.\n",
    "\n",
    "#### Twitter Archive Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the twitter archive table\n",
    "twitter_archive.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Indentified issues:\n",
    "- Missing values for columns: (retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, in_reply_to_status_id, in_reply_to_user_id).\n",
    "- There is no information about the type of dog, 'None' value for columns: (doggo, floofer, pupper, puppo).\n",
    "- `source` column has embedded HTML code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`twitter_archive` columns:\n",
    "- **tweet_id**: is the tweet identifier.\n",
    "- **in_reply_to_status_id**: reply status identifier.\n",
    "- **in_reply_to_user_id**: reply to user identifier.\n",
    "- **timestamp** the timestamp assigned to each tweet when it is created.\n",
    "- **source**: the source (mobile) from which the tweet was made.\n",
    "- **text**: the text (body) that each tweet has (during that time Twitter only allowed texts of 140 characters max).\n",
    "- **retweeted_status_id**: identifier of the status if this tweet was retweeted.\n",
    "- **retweeted_status_user_id**: identifier of the user if this tweet was retweeted.\n",
    "- **retweeted_status_timestamp**: the timestamp given if this tweet was retweeted.\n",
    "- **expanded_urls**: expanded urls.\n",
    "- **rating_numerator**: numerator of the ranking, almost always above 10.\n",
    "- **rating_denominator**: denominator of the ranking, fixed to 10.\n",
    "- **name**: the given name to the dog.\n",
    "- **doggo**: a type of dog. A big pupper, usually older. This label does not stop a doggo from behaving like a pupper.\n",
    "- **floofer**: a type of dog. A any dog really. However, this label is commonly given to dogs with semmingly excess fur.\n",
    "- **pupper**: a type of dog. A small doggo, usually younger. Can be equally, if not more mature than some doggos.\n",
    "- **puppo**: a type of dog. A transitional state between pupper and doggo. Easily understood as the dog equivalent of a teenager.\n",
    "\n",
    "**Definitions for dogs were taken from:** *TheDogtionary (via the #WeRateDogs book on Amazon)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the image predictions table\n",
    "image_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Indentified issues:\n",
    "- Some breeds are capitalized while others not, also instead of a space the character '_' is used, for columns: (p1, p2, p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`image_predictions` columns:\n",
    "- **tweet_id**: is the tweet identifier.\n",
    "- **jpg_url**: the url to the image of the dog used for the prediction.\n",
    "- **img_num**: number of image.\n",
    "- **p1**: p1 is the algorithm's #1 prediction for the image in the tweet.\n",
    "- **p1_conf**: p1_conf is how confident the algorithm is in its #1 prediction.\n",
    "- **p1_dog**: p1_dog is whether or not the #1 prediction is a breed of dog.\n",
    "- **p2**: p2 is the algorithm's second most likely prediction.\n",
    "- **p2_conf**: p2_conf is how confident the algorithm is in its #2 prediction.\n",
    "- **p2_dog**: p2_dog is whether or not the #2 prediction is a breed of dog.\n",
    "- **p3**: p3 is the algorithm's third most likely prediction.\n",
    "- **p3_conf**: p2_conf is how confident the algorithm is in its #3 prediction.\n",
    "- **p3_dog**: p2_dog is whether or not the #3 prediction is a breed of dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the tweets counts table\n",
    "tweets_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tweets_counts` columns:\n",
    "- **tweet_id**: is the tweet identifier.\n",
    "- **retweet_count**: number of retweets of that tweet.\n",
    "- **favorite_count**: number of favourites of that tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatic Assesment\n",
    "Now programmatically we will look for quality and tidiness issues in the datasets, using pandas functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look for dimensions\n",
    "twitter_archive.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Indentified issues:\n",
    "- Remove retweets\n",
    "- These columns are not needed since they are retweets: (retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, in_reply_to_status_id, in_reply_to_user_id).\n",
    "- Timestamp columns as strings, not datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now all the denominators are fixed to 10\n",
    "twitter_archive.rating_denominator.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The previous information is pretty interesting, leets look further into these values, since is our main metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at those tweets with different denominators\n",
    "twitter_archive[twitter_archive.rating_denominator < 10][['tweet_id', 'text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Something seems odd about text and the numerators, leets look at them\n",
    "texts = twitter_archive[twitter_archive.rating_denominator < 10]['text'].values\n",
    "for i in range(len(texts)):\n",
    "    print(\"Text \" + str(i+1)+ \":\" + texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From these tweets we can detect what was the correct rating. Except for the case of Sam, which actually refers that she smiles all the day.\n",
    "- Tweets (835246439529840640, 666287406224695296) [13/10, 9/10] - Incorrect rating\n",
    "- Tweet (810984652412424192) - Invalid rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter_archive[twitter_archive.rating_denominator > 100][['tweet_id', 'text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Both numerator and denominator seem to be pretty high, lets look at the text to see why.\n",
    "# Something seems odd about text and the numerators, leets look at them\n",
    "texts = twitter_archive[twitter_archive.rating_denominator > 100]['text'].values\n",
    "for i in range(len(texts)):\n",
    "    print(\"Text \" + str(i+1)+ \":\" + texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If we look at each one of these tweets, we realize that the score given is for all the dogs that we can observe (who are represented by the denominator * 10)\n",
    ">\n",
    "> - Tweets (758467244762497024, 731156023742988288, 684225744407494656, 684222868335505415, 677716515794329600) - Adjust rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive[(twitter_archive.rating_denominator > 10) & \n",
    "                (twitter_archive.rating_denominator < 100)][['tweet_id', 'text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both numerator and denominator seem to be pretty high, lets look at the text to see why.\n",
    "# Something seems odd about text and the numerators, leets look at them\n",
    "texts = twitter_archive[(twitter_archive.rating_denominator > 10) & \n",
    "                (twitter_archive.rating_denominator < 100)]['text'].values\n",
    "for i in range(len(texts)):\n",
    "    print(\"Text \" + str(i+1)+ \":\" + texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Fact: Dogs who died at the terrorist attack of 9/11 are rated wrong, instead they are granted with one of the highest grades possible: 14/10. This is pretty sad :(**  \n",
    ">\n",
    "> - Tweets: (775096608509886464, 740373189193256964, 716439118184652801, 682962037429899265, 722974582966214656) [14/10, 14/10, 11/10, 10/10, 13/10] - Incorrect rating\n",
    ">\n",
    "> - Some \"ratings\" are confused with the date when an account was created. \n",
    "> Tweet: (832088576586297345) - Invalid rating\n",
    ">\n",
    "> - Same issue as the one that was mentioned for some tweets that involve 2 or more dogs. Tweets: (820690176645140481, 713900603437621249, 710658690886586372, 709198395643068416, 704054845121142784, 697463031882764288, 675853064436391936) - Adjust rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look a the more extreme outliers\n",
    "twitter_archive[twitter_archive.rating_numerator > 400][['tweet_id', 'text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making aside the tweet that was previously analized, the others seem to have normal denominators, lets see why\n",
    "texts = twitter_archive[twitter_archive.rating_numerator > 400]['text'].values\n",
    "for i in range(len(texts)):\n",
    "    print(\"Text \" + str(i+1)+ \":\" + texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at these tweets we realize something pretty funny about all of them, the one with 1776/10 makes reference to the independence year of USA, the one with 666 makes reference to the \"number of the beast\", and the ones with 420 are not dogs, but the rapper Snop Dog xD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since they are special cases, we might consider to remove them, or do not take them into account for further analysis.\n",
    "> Tweets: (855862651834028034, 855860136149123072, 749981277374128128, 670842764863651840)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note: Using NLP for the tweets who present abnormalities is an interesenting option, in order to reduce the time and automatize this activity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we will look at the types of dogs and their names\n",
    "twitter_archive.name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This column presents come problems and it will involve just too much time cleaning it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive.doggo.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter_archive.pupper.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive.floofer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter_archive.puppo.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Given the values of the previous columns, it will be better to take it as a integer column with 0's and 1's, instead of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will see if have duplicates id in the dataset.\n",
    "twitter_archive.tweet_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It seems that for this dataset we do not have duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look for dimensions\n",
    "image_predictions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - At first sight it seems that there are no missing values, and that each data type is correct.\n",
    "> - The columns are not informative enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now lets verify if there are not duplicated tweets\n",
    "image_predictions.tweet_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We verify the values of the breed columns\n",
    "print(\"p1:\" + str(image_predictions.p1.nunique()))\n",
    "print(\"p2:\" + str(image_predictions.p2.nunique()))\n",
    "print(\"p3:\" + str(image_predictions.p3.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We verify which values are in the column p1\n",
    "image_predictions.p1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are values that are not breeds and correspond to objects, not dogs. We need to be aware from these value when we select type of breed from p1, p2 or p3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets look at duplicated urls\n",
    "print(image_predictions[image_predictions.jpg_url.duplicated()].count())\n",
    "image_predictions[image_predictions.jpg_url.duplicated(keep = False)].sort_values(\n",
    "    by = 'jpg_url').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we verify those url in the twitter archive and try to guess why they are equal\n",
    "twitter_archive[twitter_archive.tweet_id.isin([675354435921575936, 752309394570878976, 842892208864923648, 807106840509214720])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we can notice that the duplicates belong to retweets, so we must do a double check to verify if they were all removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look for dimensions\n",
    "tweets_counts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now look for duplicated tweets id\n",
    "tweets_counts.tweet_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_counts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we will look at the values distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will show two plots, one without limits, and other with limit at x=10,000\n",
    "plt.figure(figsize=[15,10])\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(data = tweets_counts, x = 'retweet_count');\n",
    "plt.title(\"Retweet Count Distribution\")\n",
    "plt.xlabel('Retweets')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.hist(data = tweets_counts, x = 'retweet_count', range=(0,10000));\n",
    "plt.title(\"Retweet Count Distribution\")\n",
    "plt.xlabel('Retweets')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.hist(data = tweets_counts, x = 'favorite_count');\n",
    "plt.title(\"Favorite Count Distribution\")\n",
    "plt.xlabel('Favorite')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(data = tweets_counts, x = 'favorite_count', range=(0,20000));\n",
    "plt.title(\"Favorite Count Distribution\")\n",
    "plt.xlabel('Favorite')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From the previous statistics and plots we can see that the distribution is right skewed, with some tweets (outliers) that we should be aware of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "### Quality\n",
    "`twitter_achive table`\n",
    "- There are rows that belong to retweets, we should get rid of this rows.\n",
    "- These columns are not needed since they belong to retweets: (retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, in_reply_to_status_id, in_reply_to_user_id).\n",
    "\n",
    "- In the name column, the name is not an actual name. There are serveral prepositions that shouldn't be there, like: \"the, an, this, etc\".\n",
    "- Incorrect datatype for the column 'timestamp', instead of string it should be datetime.\n",
    "- Incorrect datatype for the columns *(doggo, floofer, pupper, puppo)* they are string when they only have two possible values, so they should be boolean datatype. \n",
    "- The rate assigned to the following tweets is incorrect, they should be corrected. Tweets (835246439529840640, 666287406224695296, 775096608509886464, 740373189193256964, 716439118184652801, 682962037429899265, 722974582966214656) [14/10, 14/10, 11/10, 10/10, 13/10, 13/10, 9/10] - Incorrect rating\n",
    "- The rate assigned to the following tweets is for all the dogs that appear on it, so the values should be recalculated. Tweets (758467244762497024, 731156023742988288, 684225744407494656, 684222868335505415, 677716515794329600, 820690176645140481, 713900603437621249, 710658690886586372, 709198395643068416, 704054845121142784, 697463031882764288, 675853064436391936) - Adjust rating\n",
    "- These tweets do not have valid ratings and there is no futher information that we can use, so they should be removed. Tweets (810984652412424192, 855862651834028034, 855860136149123072, 749981277374128128, 670842764863651840 )\n",
    "\n",
    "`image_predictions table`\n",
    "- Columns: *(p1, p1_conf, p1_dog, p2, p2_conf, p2_dog, p3, p3_conf, p3_dog)* are not informative enough.\n",
    "- In the columns *(p1, p2, p3)* all the values should be converted to lower case to avoid misspelling.\n",
    "\n",
    "### Tidiness\n",
    "- There are 2075 rows in `image_prediction table`, 2356 in `tweet_archive table`, and 2354 `tweets_count table`. \n",
    "- The types of dog corresponds to only one variable which is called the age stage of that dog.\n",
    "- At the end we only need 1 a table that contains all the data relevant for our analysis (observational unit).\n",
    "- In `image_prediction table` we only need 1 column for the breed and the result, instead of the 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning'></a>\n",
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a copy of the three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all we make a copy of the three datasets before we start cleaning\n",
    "twitter_archivecp = twitter_archive.copy()\n",
    "image_predictionscp = image_predictions.copy()\n",
    "tweets_countscp = tweets_counts.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "\n",
    "`twitter_archivecp`\n",
    "\n",
    "#### Define\n",
    "- Remove all the rows that belong to retweets.\n",
    "- Remove columns that are associated with retweets (retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, in_reply_to_status_id, in_reply_to_user_id).\n",
    "- Remove all the rows that contain tweets that have no valid ratings: (810984652412424192, 855862651834028034, 855860136149123072, 749981277374128128, 670842764863651840 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we obtain all the tweets that have null values in the parameters that belong to retweets\n",
    "twitter_archivecp = twitter_archivecp[(twitter_archivecp.retweeted_status_user_id.isnull()) & \n",
    "                (twitter_archivecp.retweeted_status_id.isnull()) & \n",
    "                (twitter_archivecp.retweeted_status_timestamp.isnull()) & \n",
    "                (twitter_archivecp.in_reply_to_status_id.isnull()) & \n",
    "                (twitter_archivecp.in_reply_to_user_id.isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all the columns that are associated with retweets\n",
    "twitter_archivecp.drop(['retweeted_status_id', 'retweeted_status_user_id',\n",
    "                      'retweeted_status_timestamp', 'in_reply_to_status_id',\n",
    "                      'in_reply_to_user_id'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we remove all the rows that have no valid strings\n",
    "invalid_tweets = [810984652412424192, 855862651834028034, 855860136149123072, 749981277374128128, 670842764863651840]\n",
    "indexes = twitter_archivecp.index[twitter_archivecp.tweet_id.isin(invalid_tweets)].tolist()\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archivecp.drop(indexes, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter_archivecp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert twitter_archivecp[twitter_archivecp.tweet_id.isin(invalid_tweets) == True].shape[0]== 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "\n",
    "- Replace all the prepositions or words that are not actual names with nulls, format the name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the words that are not names\n",
    "non_name = ['O', 'a', 'a', 'about', 'above', 'after', 'again', \n",
    "             'against', 'all', 'all', 'am', 'an', 'an', 'and', \n",
    "             'any', 'are', 'as', 'at', 'at', 'be', 'because', \n",
    "             'been', 'before', 'being', 'below', 'between', \n",
    "             'both', 'but', 'by', 'by', 'can', 'did', 'do', \n",
    "             'does', 'doing', 'don', 'down', 'during', 'each', \n",
    "             'few', 'for', 'from', 'further', 'had', 'has', \n",
    "             'have', 'having', 'he', 'her', 'here', 'hers', \n",
    "             'herself', 'him', 'himself', 'his', 'how', 'i', \n",
    "             'if', 'in', 'into', 'is', 'it', 'its', 'itself', \n",
    "             'just', 'just', 'life', 'light', 'me', 'more', \n",
    "             'most', 'my', 'my', 'myself', 'no', 'nor', 'not', \n",
    "             'not', 'now', 'none', 'of', 'off', 'old', 'on', 'once', \n",
    "             'only', 'or', 'other', 'our', 'ours', 'ourselves', \n",
    "             'out', 'over', 'own', 'quite', 's', 'same', 'she', \n",
    "             'should', 'so', 'some', 'space', 'such', 'such', \n",
    "             't', 'than', 'that', 'the', 'the', 'their', 'theirs', \n",
    "             'them', 'themselves', 'then', 'there', 'these', 'they', \n",
    "             'this', 'this', 'those', 'through', 'to', 'too', 'under', \n",
    "             'until', 'up', 'very', 'very', 'was', 'we', 'were', \n",
    "             'what', 'when', 'where', 'which', 'while', 'who', \n",
    "             'whom', 'why', 'will', 'with', 'you', 'your', 'yours', \n",
    "             'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the non name words with nulls and capitalize the first letter\n",
    "twitter_archivecp.name = twitter_archivecp.name.str.lower()\n",
    "twitter_archivecp.name.replace(non_name, np.nan, inplace = True)\n",
    "twitter_archivecp.name = twitter_archivecp.name.str.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archivecp.name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Convert to datetime the datatype of the column 'timestamp'\n",
    "- Convert to boolean (integer values of 0 and 1) datatype the columns 'doggo', 'floofer', 'pupper', 'puppo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime the column timestamp\n",
    "twitter_archivecp.timestamp = pd.to_datetime(twitter_archivecp.timestamp, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to boolean the columns 'doggo', 'floofer', 'pupper', 'puppo'\n",
    "columns = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "for column in columns:\n",
    "    twitter_archivecp[column] = twitter_archivecp[column].replace({column:1, 'None':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We verify that the convertion was sucessful\n",
    "twitter_archivecp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Correct the ratings for the following tweets: (835246439529840640, 666287406224695296, 775096608509886464, 740373189193256964, 716439118184652801, 682962037429899265, 722974582966214656). Ratings: [14/10, 14/10, 11/10, 10/10, 13/10, 13/10, 9/10]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all we have to verify which tweets were not deleted (retweets) and the we proceed to correct the ratings\n",
    "verify_tweets = [835246439529840640, 666287406224695296, 775096608509886464, 740373189193256964, 716439118184652801, 682962037429899265, 722974582966214656]\n",
    "\n",
    "twitter_archivecp[twitter_archivecp.tweet_id.isin(verify_tweets) == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the ones who were not deleted we proceed to adjust the ratings\n",
    "adjust_tweets = [740373189193256964, 722974582966214656, 716439118184652801, 682962037429899265, 666287406224695296]\n",
    "ratings = [[10,10], [9,10], [13,10], [13,10], [14,10]]\n",
    "indexes = twitter_archivecp.index[twitter_archivecp.tweet_id.isin(adjust_tweets)].tolist()\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(indexes)):\n",
    "    twitter_archivecp.loc[indexes[i], 'rating_numerator'] = ratings[i][0]\n",
    "    twitter_archivecp.loc[indexes[i], 'rating_denominator'] = ratings[i][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we verify that the values had been replaced\n",
    "twitter_archivecp[twitter_archivecp.tweet_id.isin(verify_tweets) == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Recalculate and correct the ratings for the following tweets: (758467244762497024, 731156023742988288, 684225744407494656, 684222868335505415, 677716515794329600, 820690176645140481, 713900603437621249, 710658690886586372, 709198395643068416, 704054845121142784, 697463031882764288, 675853064436391936)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all we have to verify which tweets were not deleted (retweets) and the we proceed to correct the ratings\n",
    "recalculate_tweets = [758467244762497024, 731156023742988288, 684225744407494656, 684222868335505415, 677716515794329600, 820690176645140481, 713900603437621249, 710658690886586372, 709198395643068416, 704054845121142784, 697463031882764288, 675853064436391936]\n",
    "\n",
    "twitter_archivecp[twitter_archivecp.tweet_id.isin(recalculate_tweets) == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we extract the indexes and we proceed to recalculate the rating values\n",
    "indexes = twitter_archivecp.index[twitter_archivecp.tweet_id.isin(recalculate_tweets)].tolist()\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We recalculate the values\n",
    "for index in indexes:\n",
    "    number_of_dogs = twitter_archivecp.loc[index, 'rating_denominator']/10\n",
    "    twitter_archivecp.loc[index, 'rating_numerator'] = int(twitter_archivecp.loc[index, 'rating_numerator']/number_of_dogs)\n",
    "    twitter_archivecp.loc[index, 'rating_denominator'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archivecp[twitter_archivecp.tweet_id.isin(recalculate_tweets) == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "\n",
    "`image_predictions table`\n",
    "- Convert values of columns (p1,p2,p3) to lowercase\n",
    "- Rename the columns (p1, p1_conf, p1_dog, p2, p2_conf, p2_dog, p3, p3_conf, p3_dog) to (prediction_#, conf_probability_#, predicted_breed_#), where # is the number 1,2 and 3.\n",
    "- Create two columns (breed, result) were the values with the highest probability will be held."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the columns 'p1', 'p2' and 'p3' to lowercase\n",
    "columns = ['p1', 'p2', 'p3']\n",
    "for column in columns:\n",
    "    image_predictionscp[column] = image_predictionscp[column].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we rename the columns (p1, p1_conf, p1_dog, p2, p2_conf, p2_dog, p3, p3_conf, p3_dog)\n",
    "image_predictionscp.rename(columns={\"p1\": \"prediction_1\", \"p2\": \"prediction:_2\", \"p3\": \"prediction:_3\", \n",
    "                                    \"p1_conf\": \"conf_probability_1\", \"p2_conf\": \"conf_probability_2\", \n",
    "                                    \"p3_conf\": \"conf_probability_3\", \"p1_dog\": \"predicted_breed_1\",\n",
    "                                    \"p2_dog\": \"predicted_breed_2\", \"p3_dog\": \"predicted_breed_3\",}, inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add a new column in which we will save the breed (in case of a dog) or object with the highest probaility\n",
    "# and another column which states that if it is a dog or not.\n",
    "# Remove all the rows in which neither of the 3 predictions are dogs\n",
    "indexes = image_predictionscp.index[((image_predictionscp.predicted_breed_1) | \n",
    "                    (image_predictionscp.predicted_breed_2) | \n",
    "                    (image_predictionscp.predicted_breed_3)) == False].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verify that there is no more duplicated urls due to retweets\n",
    "# Now lets look at duplicated urls\n",
    "indexes = image_predictionscp[image_predictionscp.jpg_url.duplicated(keep = False)].sort_values(\n",
    "    by = 'jpg_url').index\n",
    "len(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictionscp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter_archivecp[twitter_archivecp.index.isin(indexes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Seems that there are not more retweets :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='iterating'></a>\n",
    "## Additional Assessing and Cleaning (Iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatic Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's look if there are any rates that are inconsistent\n",
    "twitter_archivecp.rating_numerator.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are 3 values that are suspicious, lets dig in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter_archivecp[twitter_archivecp.rating_numerator > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = twitter_archivecp[twitter_archivecp.rating_numerator > 20]['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(texts)):\n",
    "    print(\"Text \" + str(i+1)+ \":\" + texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reading at the text we notice that the problem was that the numerators are the decimal values of the rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality\n",
    "`twitter_archivecp`\n",
    "- The rate assigned to the following tweets is incorrect, they should be corrected. Tweets: (786709082849828864, 778027034220126208, 680494726643068929) [10/10, 11/10, 11/10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "Correct the ratings for the following tweets: (786709082849828864, 778027034220126208, 680494726643068929). Ratings: [10/10, 11/10, 11/10]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the ones who were not deleted we proceed to adjust the ratings\n",
    "adjust_tweets = [786709082849828864, 778027034220126208, 680494726643068929]\n",
    "ratings = [[10,10], [11,10], [11,10]]\n",
    "indexes = twitter_archivecp.index[twitter_archivecp.tweet_id.isin(adjust_tweets)].tolist()\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(indexes)):\n",
    "    twitter_archivecp.loc[indexes[i], 'rating_numerator'] = ratings[i][0]\n",
    "    twitter_archivecp.loc[indexes[i], 'rating_denominator'] = ratings[i][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we verify that the values had been replaced\n",
    "twitter_archivecp[twitter_archivecp.tweet_id.isin(adjust_tweets) == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Group the different types of doggs into only one column named age stage which will contain all the stages of the dog's age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each stage of the dog that exists: doggo, floofer, puper, puppo\n",
    "age_stages = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "# Function that maps every state of the dogs\n",
    "def calc_dog_stage(stages):\n",
    "    for stage in stages:\n",
    "        if stage in age_stages:\n",
    "            return stage\n",
    "        else:\n",
    "            pass\n",
    "    return stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the given function\n",
    "twitter_archivecp['age_stages'] = twitter_archivecp[['doggo', 'floofer',  'pupper', 'puppo']].apply(calc_dog_stage, axis = 1)\n",
    "# Then we drop the remaining columns\n",
    "twitter_archivecp.drop(['doggo', 'floofer', 'pupper', 'puppo'], \n",
    "    inplace = True, axis = 1)\n",
    "# Finally we convert that variable as category\n",
    "twitter_archivecp.age_stages = twitter_archivecp.age_stages.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  We verify that the previous operation was sucessfull\n",
    "twitter_archivecp.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- In `image_prediction table` create two arrays one with the prediction, and other with the result of the predictions that had the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a two list, one with the object and other with the result\n",
    "predictions =[]\n",
    "results = []\n",
    "\n",
    "# For loop, were the values with the highest probability are selected and appended\n",
    "for i in range (image_predictionscp.shape[0]):\n",
    "    prob1 = image_predictionscp.loc[i]['conf_probability_1']\n",
    "    prob2 = image_predictionscp.loc[i]['conf_probability_2']\n",
    "    prob3 = image_predictionscp.loc[i]['conf_probability_3']\n",
    "    \n",
    "    if (prob1 > prob2) and (prob1 > prob3):\n",
    "        predictions.append(image_predictionscp.loc[i]['prediction_1'])\n",
    "        results.append(image_predictionscp.loc[i]['predicted_breed_1'])\n",
    "    elif (prob2 > prob1) and (prob2 > prob3):\n",
    "        predictions.append(image_predictionscp.loc[i]['prediction_2'])\n",
    "        results.append(image_predictionscp.loc[i]['predicted_breed_2'])\n",
    "    else:\n",
    "        predictions.append(image_predictionscp.loc[i]['prediction_3'])\n",
    "        results.append(image_predictionscp.loc[i]['predicted_breed_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the columns: 'predictions' and 'results'\n",
    "image_predictionscp['predictions'] = predictions\n",
    "image_predictionscp['results'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>jpg_url</th>\n",
       "      <th>img_num</th>\n",
       "      <th>prediction_1</th>\n",
       "      <th>conf_probability_1</th>\n",
       "      <th>predicted_breed_1</th>\n",
       "      <th>prediction:_2</th>\n",
       "      <th>conf_probability_2</th>\n",
       "      <th>predicted_breed_2</th>\n",
       "      <th>prediction:_3</th>\n",
       "      <th>conf_probability_3</th>\n",
       "      <th>predicted_breed_3</th>\n",
       "      <th>predictions</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>666020888022790149</td>\n",
       "      <td>https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>welsh_springer_spaniel</td>\n",
       "      <td>0.465074</td>\n",
       "      <td>True</td>\n",
       "      <td>collie</td>\n",
       "      <td>0.156665</td>\n",
       "      <td>True</td>\n",
       "      <td>shetland_sheepdog</td>\n",
       "      <td>0.061428</td>\n",
       "      <td>True</td>\n",
       "      <td>welsh_springer_spaniel</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>666029285002620928</td>\n",
       "      <td>https://pbs.twimg.com/media/CT42GRgUYAA5iDo.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>redbone</td>\n",
       "      <td>0.506826</td>\n",
       "      <td>True</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "      <td>0.074192</td>\n",
       "      <td>True</td>\n",
       "      <td>rhodesian_ridgeback</td>\n",
       "      <td>0.072010</td>\n",
       "      <td>True</td>\n",
       "      <td>redbone</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>666033412701032449</td>\n",
       "      <td>https://pbs.twimg.com/media/CT4521TWwAEvMyu.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>german_shepherd</td>\n",
       "      <td>0.596461</td>\n",
       "      <td>True</td>\n",
       "      <td>malinois</td>\n",
       "      <td>0.138584</td>\n",
       "      <td>True</td>\n",
       "      <td>bloodhound</td>\n",
       "      <td>0.116197</td>\n",
       "      <td>True</td>\n",
       "      <td>german_shepherd</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>666044226329800704</td>\n",
       "      <td>https://pbs.twimg.com/media/CT5Dr8HUEAA-lEu.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>rhodesian_ridgeback</td>\n",
       "      <td>0.408143</td>\n",
       "      <td>True</td>\n",
       "      <td>redbone</td>\n",
       "      <td>0.360687</td>\n",
       "      <td>True</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "      <td>0.222752</td>\n",
       "      <td>True</td>\n",
       "      <td>rhodesian_ridgeback</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>666049248165822465</td>\n",
       "      <td>https://pbs.twimg.com/media/CT5IQmsXIAAKY4A.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "      <td>0.560311</td>\n",
       "      <td>True</td>\n",
       "      <td>rottweiler</td>\n",
       "      <td>0.243682</td>\n",
       "      <td>True</td>\n",
       "      <td>doberman</td>\n",
       "      <td>0.154629</td>\n",
       "      <td>True</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                          jpg_url  \\\n",
       "0  666020888022790149  https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg   \n",
       "1  666029285002620928  https://pbs.twimg.com/media/CT42GRgUYAA5iDo.jpg   \n",
       "2  666033412701032449  https://pbs.twimg.com/media/CT4521TWwAEvMyu.jpg   \n",
       "3  666044226329800704  https://pbs.twimg.com/media/CT5Dr8HUEAA-lEu.jpg   \n",
       "4  666049248165822465  https://pbs.twimg.com/media/CT5IQmsXIAAKY4A.jpg   \n",
       "\n",
       "   img_num            prediction_1  conf_probability_1  predicted_breed_1  \\\n",
       "0        1  welsh_springer_spaniel            0.465074               True   \n",
       "1        1                 redbone            0.506826               True   \n",
       "2        1         german_shepherd            0.596461               True   \n",
       "3        1     rhodesian_ridgeback            0.408143               True   \n",
       "4        1      miniature_pinscher            0.560311               True   \n",
       "\n",
       "        prediction:_2  conf_probability_2  predicted_breed_2  \\\n",
       "0              collie            0.156665               True   \n",
       "1  miniature_pinscher            0.074192               True   \n",
       "2            malinois            0.138584               True   \n",
       "3             redbone            0.360687               True   \n",
       "4          rottweiler            0.243682               True   \n",
       "\n",
       "         prediction:_3  conf_probability_3  predicted_breed_3  \\\n",
       "0    shetland_sheepdog            0.061428               True   \n",
       "1  rhodesian_ridgeback            0.072010               True   \n",
       "2           bloodhound            0.116197               True   \n",
       "3   miniature_pinscher            0.222752               True   \n",
       "4             doberman            0.154629               True   \n",
       "\n",
       "              predictions  results  \n",
       "0  welsh_springer_spaniel     True  \n",
       "1                 redbone     True  \n",
       "2         german_shepherd     True  \n",
       "3     rhodesian_ridgeback     True  \n",
       "4      miniature_pinscher     True  "
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We visualize the creation of the columns\n",
    "image_predictionscp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "\n",
    "- Combine the 3 tables together into a single table named: `twitter_archive_master.csv`.\n",
    "- Drop the columns that will be not part of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_master = twitter_archivecp.merge(tweets_countscp, on = 'tweet_id', how = 'left')\n",
    "twitter_archive_master = twitter_archive_master.merge(image_predictionscp[['tweet_id','jpg_url', 'img_num', 'predictions', 'results']], \n",
    "  on = 'tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_master.drop(['source', 'text', 'expanded_urls'], \n",
    "  inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1968 entries, 0 to 1967\n",
      "Data columns (total 12 columns):\n",
      "tweet_id              1968 non-null int64\n",
      "timestamp             1968 non-null datetime64[ns]\n",
      "rating_numerator      1968 non-null int64\n",
      "rating_denominator    1968 non-null int64\n",
      "name                  1358 non-null object\n",
      "age_stages            1968 non-null category\n",
      "retweet_count         1968 non-null int64\n",
      "favorite_count        1968 non-null int64\n",
      "jpg_url               1968 non-null object\n",
      "img_num               1968 non-null int64\n",
      "predictions           1968 non-null object\n",
      "results               1968 non-null bool\n",
      "dtypes: bool(1), category(1), datetime64[ns](1), int64(6), object(3)\n",
      "memory usage: 173.1+ KB\n"
     ]
    }
   ],
   "source": [
    "twitter_archive_master.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>rating_numerator</th>\n",
       "      <th>rating_denominator</th>\n",
       "      <th>name</th>\n",
       "      <th>age_stages</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>jpg_url</th>\n",
       "      <th>img_num</th>\n",
       "      <th>predictions</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892420643555336193</td>\n",
       "      <td>2017-08-01 16:23:56</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>Phineas</td>\n",
       "      <td>0</td>\n",
       "      <td>8853</td>\n",
       "      <td>39467</td>\n",
       "      <td>https://pbs.twimg.com/media/DGKD1-bXoAAIAUK.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>orange</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>892177421306343426</td>\n",
       "      <td>2017-08-01 00:17:27</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>Tilly</td>\n",
       "      <td>0</td>\n",
       "      <td>6514</td>\n",
       "      <td>33819</td>\n",
       "      <td>https://pbs.twimg.com/media/DGGmoV4XsAAUL6n.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>chihuahua</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>891815181378084864</td>\n",
       "      <td>2017-07-31 00:18:03</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>Archie</td>\n",
       "      <td>0</td>\n",
       "      <td>4328</td>\n",
       "      <td>25461</td>\n",
       "      <td>https://pbs.twimg.com/media/DGBdLU1WsAANxJ9.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>chihuahua</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>891689557279858688</td>\n",
       "      <td>2017-07-30 15:58:51</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>Darla</td>\n",
       "      <td>0</td>\n",
       "      <td>8964</td>\n",
       "      <td>42908</td>\n",
       "      <td>https://pbs.twimg.com/media/DF_q7IAWsAEuuN8.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>paper_towel</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>891327558926688256</td>\n",
       "      <td>2017-07-29 16:00:24</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>Franklin</td>\n",
       "      <td>0</td>\n",
       "      <td>9774</td>\n",
       "      <td>41048</td>\n",
       "      <td>https://pbs.twimg.com/media/DF6hr6BUMAAzZgT.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>basset</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id           timestamp  rating_numerator  \\\n",
       "0  892420643555336193 2017-08-01 16:23:56                13   \n",
       "1  892177421306343426 2017-08-01 00:17:27                13   \n",
       "2  891815181378084864 2017-07-31 00:18:03                12   \n",
       "3  891689557279858688 2017-07-30 15:58:51                13   \n",
       "4  891327558926688256 2017-07-29 16:00:24                12   \n",
       "\n",
       "   rating_denominator      name age_stages  retweet_count  favorite_count  \\\n",
       "0                  10   Phineas          0           8853           39467   \n",
       "1                  10     Tilly          0           6514           33819   \n",
       "2                  10    Archie          0           4328           25461   \n",
       "3                  10     Darla          0           8964           42908   \n",
       "4                  10  Franklin          0           9774           41048   \n",
       "\n",
       "                                           jpg_url  img_num  predictions  \\\n",
       "0  https://pbs.twimg.com/media/DGKD1-bXoAAIAUK.jpg        1       orange   \n",
       "1  https://pbs.twimg.com/media/DGGmoV4XsAAUL6n.jpg        1    chihuahua   \n",
       "2  https://pbs.twimg.com/media/DGBdLU1WsAANxJ9.jpg        1    chihuahua   \n",
       "3  https://pbs.twimg.com/media/DF_q7IAWsAEuuN8.jpg        1  paper_towel   \n",
       "4  https://pbs.twimg.com/media/DF6hr6BUMAAzZgT.jpg        2       basset   \n",
       "\n",
       "   results  \n",
       "0    False  \n",
       "1     True  \n",
       "2     True  \n",
       "3    False  \n",
       "4     True  "
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_archive_master.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the data\n",
    "\n",
    "Finally we have our dataset cleaned and ready to be stored, we will use SQL Lite as DBMS to create a database and store `twitter_archive table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, we create a backup for the dataframes cleaned\n",
    "folder_name = 'bck'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Twitter Archive Clenaned Dataframe\n",
    "twitter_archivecp.to_csv('bck/twitter_archivecp.csv', encoding='utf-8', index=False)\n",
    "# Image Predictions Clenaned Dataframe\n",
    "image_predictionscp.to_csv('bck/image_predictionscp.csv', encoding='utf-8', index=False)\n",
    "# Tweets Counts Cleaned Dataframe\n",
    "tweets_countscp.to_csv('bck/tweets_countscp.csv', encoding='utf-8', index=False)\n",
    "# Twitter Archive Master Clenaned Dataframe\n",
    "twitter_archive_master.to_csv('bck/twitter_archive_master.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we save the master dataframe in disk, using a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the sqlite database\n",
    "database = 'wrangle_act.db'\n",
    "conn = sqlite3.connect(database)\n",
    "twitter_archive_master.to_sql('bck/twitter_archive_master', conn, if_exists = 'replace', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally we test the database, by making a select statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * FROM twitter_archive_master': no such table: twitter_archive_master",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1430\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1431\u001b[1;33m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1432\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: twitter_archive_master",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-348-8c9cd51b064d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Make sure they all read back from the database\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtwitter_archive_master\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SELECT * FROM twitter_archive_master'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m             \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             chunksize=chunksize)\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_query\u001b[1;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[0;32m   1466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1467\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1468\u001b[1;33m         \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1469\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1443\u001b[0m                 \"Execution failed on sql '{sql}': {exc}\".format(\n\u001b[0;32m   1444\u001b[0m                     sql=args[0], exc=exc))\n\u001b[1;32m-> 1445\u001b[1;33m             \u001b[0mraise_with_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1447\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\compat\\__init__.py\u001b[0m in \u001b[0;36mraise_with_traceback\u001b[1;34m(exc, traceback)\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;31m# this version of raise is a syntax error in Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1429\u001b[0m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1430\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1431\u001b[1;33m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1432\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1433\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT * FROM twitter_archive_master': no such table: twitter_archive_master"
     ]
    }
   ],
   "source": [
    "# Make sure they all read back from the database\n",
    "twitter_archive_master = pd.read_sql('SELECT * FROM twitter_archive_master', conn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
